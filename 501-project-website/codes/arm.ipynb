{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARM and Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rt littlebirbmame aungaungnumber ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rt littlebirbmame yamyummy ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏Å‡∏î‡∏≤‡∏ô‡∏°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt angel_packsss üéâ promo√ß√µes que est√£o rolando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>rt wayydaminn love privacy worth share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>rt sapphnumberc hawaii island small population...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                  0\n",
       "0           0  rt littlebirbmame aungaungnumber ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤...\n",
       "1           1  rt littlebirbmame yamyummy ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏Å‡∏î‡∏≤‡∏ô‡∏°...\n",
       "2           2  rt angel_packsss üéâ promo√ß√µes que est√£o rolando...\n",
       "3           3             rt wayydaminn love privacy worth share\n",
       "4           4  rt sapphnumberc hawaii island small population..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/yangyilin/Desktop/anly-501-project-YilinYang2000-1/data/00-raw-data/tweetresult.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL AVERAGE SENTEMENT: [0.06511549 0.80322254 0.12770817]\n",
      "VOCAB LENGTH 4059\n",
      "[['enter', 'rt', 'tweet', 'amp', 'tag', 'timpithenewway', 'amp', 'add', 'hashtag', 'timpipositive185', '172rt', 'honeymoonluve', 'gosta', 'da', 'viso'], ['set', 'desse', 'dia', 'com', 'muita', 'buceta', 'e', 'cuinho', 'l', 'privacy', 'view'], ['set', 'day', 'witpositive95', '173rt', 'littlebirbmame', 'yamyummy', 'unicef', 'un', 'privacy', 'privacy', 'neutral1000', '174rt', 'bblivery', 'privacy', 'neutral1000', '175rt', 'mahomet9', 'cant', 'even', 'stress', 'enough', 'privacy', 'crypto', 'active', 'process', 'constantly', 'work', 'maintain'], ['neutral784', '176rt', 'annabrennet', 'quartou', 'amores', 'privacy', 'atualiado', 'com', 'novo', 'pack', 'scat', '10', 'min', 'novos', 'assinantes', 'ganham', '15', 'min', 'de', 'chamada', '3', 'pack', 'rneutral1000', '177rt', 'johnmagrinho', 'kero', 'socar', 'um', 'cu', 'faer', 'de', 'buceta', 'bater', 'leite', 'espumar', 'quem', 'se', 'habilita'], ['assista', 'meus', 'vdeos', 'completos', '100', 'barneutral1000', '178rt', 'novinhosfut', 'httpstcoofboqhsxrd', 'httpstcoktdbrutodxneutral1000', '179rt', 'ksspy', 'defend', 'privacy', 'neutral1000', '180rt', 'oteuporrete23', 'aguenta', 'da', 'cabea', 'saco']]\n"
     ]
    }
   ],
   "source": [
    "#USER PARAM\n",
    "input_path\t\t\t=\t'/Users/yangyilin/Desktop/anly-501-project-YilinYang2000-1/data/00-raw-data/pytweetresult.csv'\n",
    "compute_sentiment \t=\tTrue\t\t\n",
    "sentiment    \t\t=\t[]\t\t\t#average sentiment of each chunck of text \n",
    "ave_window_size\t\t=\t250\t\t\t#size of scanning window for moving average\n",
    "\t\t\t\t\t\n",
    "\n",
    "#OUTPUT FILE\n",
    "output='transactions.txt'\n",
    "if os.path.exists(output): os.remove(output)\n",
    "\n",
    "#INITIALIZE\n",
    "lemmatizer \t= \tWordNetLemmatizer()\n",
    "ps \t\t\t=\tPorterStemmer()\n",
    "sia \t\t= \tSentimentIntensityAnalyzer()\n",
    "\n",
    "#ADD MORE\n",
    "stopwords\t=\tstopwords.words('english')\n",
    "add=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\n",
    "for sp in add: stopwords.append(sp)\n",
    "\n",
    "def read_and_clean(path,START=0,STOP=-1):\n",
    "\tglobal sentiment \n",
    "\n",
    "\t#-----------------------\n",
    "\t#INSERT CODE TO READ IN AS ONE BIG STING\n",
    "\tfile = open(path, 'rt')\n",
    "\ttext = file.read().lower()\n",
    "\tfile.close()\n",
    "\t#-----------------------\n",
    "\n",
    "\t#REMOVE HEADER, AND NEW LINES\n",
    "\ttext=text.replace(\"'\",'') #wasn't --> wasnt\n",
    "\tlines = text.splitlines(); text=''; \n",
    "\tlines=lines[START:STOP]    # mystring.replace('\\n', ' ')\n",
    "\tfor line in lines: text=text+' '+line\n",
    "\n",
    "\t#-----------------------\n",
    "\t#INSERT CODE TO ONLY KEEP CHAR IN string.printable\n",
    "\ttmp = ''; printable = set(string.printable)\n",
    "\tfor char in text:\n",
    "\t\tif(char in printable): tmp = tmp+char\n",
    "\ttext = tmp\n",
    "\t#-----------------------\n",
    "\n",
    "\t#BREAK INTO CHUNKS (SENTANCES OR OTHERWISE)\n",
    "\tsentences=nltk.tokenize.sent_tokenize(text)  #SENTENCES\n",
    "\n",
    "\t#CLEAN AND LEMMATIZE\n",
    "\tkeep='0123456789abcdefghijklmnopqrstuvwxy';\n",
    "\n",
    "\tnew_sentences=[]; vocabulary=[]\n",
    "\tfor sentence in sentences:\n",
    "\t\tnew_sentence=''\n",
    "\n",
    "\t\t# REBUILD LEMITIZED SENTENCE\n",
    "\t\tfor word in sentence.split():\n",
    "\t\t\t\n",
    "\t\t\t#ONLY KEEP CHAR IN \"keep\"\n",
    "\t\t\ttmp2=''\n",
    "\t\t\tfor char in word: \n",
    "\t\t\t\tif(char in keep): \n",
    "\t\t\t\t\ttmp2=tmp2+char\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttmp2=tmp2+' '\n",
    "\t\t\tword=tmp2\n",
    "\n",
    "\t\t\t#-----------------------\n",
    "\t\t\t# INSERT CODE TO LEMMATIZE THE WORDS\n",
    "\t\t\tword = tmp2\n",
    "\t\t\tnew_word = lemmatizer.lemmatize(word)\n",
    "\t\t\t#-----------------------\n",
    "\n",
    "\t\t\t#REMOVE WHITE SPACES\n",
    "\t\t\tnew_word=new_word.replace(' ', '')\n",
    "\n",
    "\t\t\t#BUILD NEW SENTANCE BACK UP\n",
    "\t\t\tif( new_word not in stopwords):\n",
    "\t\t\t\tif(new_sentence==''):\n",
    "\t\t\t\t\tnew_sentence=new_word\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnew_sentence=new_sentence+','+new_word\n",
    "\t\t\t\tif(new_word not in vocabulary): vocabulary.append(new_word)\n",
    "\n",
    "\t\t#SAVE (LIST OF LISTS)\t\t\n",
    "\t\tnew_sentences.append(new_sentence.split(\",\"))\n",
    "\t\t\n",
    "\t\t#SIA\n",
    "\t\tif(compute_sentiment):\n",
    "\t\t\t#-----------------------\n",
    "\t\t\t# INSERT CODE TO USE NLTK TO DO SENTIMENT ANALYSIS \n",
    "\t\t\ttext1 = new_sentence.replace(',', ' ')\n",
    "\t\t\tscore = sia.polarity_scores(text1)\n",
    "\t\t\tsentiment.append([score['neg'], score['neu'], score['compound']])\n",
    "\t\t\t#-----------------------\n",
    "\t\t\t\n",
    "\t\t#SAVE SENTANCE TO OUTPUT FILE\n",
    "\t\tif(len(new_sentence.split(','))>2):\n",
    "\t\t\tf = open(output, \"a\")\n",
    "\t\t\tf.write(new_sentence+\"\\n\")\n",
    "\t\t\tf.close()\n",
    "\n",
    "\tsentiment=np.array(sentiment)\n",
    "\tprint(\"TOTAL AVERAGE SENTEMENT:\",np.mean(sentiment,axis=0))\n",
    "\tprint(\"VOCAB LENGTH\",len(vocabulary))\n",
    "\treturn new_sentences\n",
    "\n",
    "transactions=read_and_clean(input_path,400,-400)\n",
    "print(transactions[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b5a992b9213609476f3b9dedb9c34ba69d78bca4613c4854b5108b11a72ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
