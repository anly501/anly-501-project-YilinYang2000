{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API\n",
    "\n",
    "Python: I used twitter API to scratch mutiple keywords:\"media\" \"internet influencers\" \"consumers\" \"data\" to gather comments from twiter about their opinion about the wemedia. I use these data to analyze the positive and negative opinion. I can detect the internet users' attitudes to consumers' privacy. I can also use this datasets to define the relationship between data and media. The keywords may not clear enough for me to analysis but I will adjust it later for future research. I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# READ FILE\n",
    "f = open(\"/Users/yangyilin/Desktop/2022-fall-anly/501/lab/lab1.2/api-keys.json\")\n",
    "input=json.load(f); #print(input)\n",
    "\n",
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Set up Connection\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the search_twitter function here.\n",
    "def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}\".format(query, max_results,tweet_fields)\n",
    "    print(\"--------------\",url,\"--------------\")\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fields = \"tweet.fields=text,author_id,created_at,lang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=media&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=internet influencer&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=data&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
      "-------------- https://api.twitter.com/2/tweets/search/recent?query=consumer privacy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n"
     ]
    }
   ],
   "source": [
    "data = \"/Users/yangyilin/Desktop/2022-fall-anly/501/project\"\n",
    "search_tweets = ['media','internet influencer','data','consumer privacy']\n",
    "for idx,val in enumerate(search_tweets):\n",
    "    tweets_jsondump = []\n",
    "    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
    "    for i in json_response1['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    for i in json_response2['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    for i in json_response3['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    for i in json_response4['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    for i in json_response5['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    for i in json_response6['data']:\n",
    "        tweets_jsondump.append(i)\n",
    "    with open(data+str(val)+'.json','w') as json_file:\n",
    "        json.dump(tweets_jsondump,json_file)\n",
    "        json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>1569873208131690497</td>\n",
       "      <td>RT @KarlBode: also for the fiftieth time the s...</td>\n",
       "      <td>2022-09-14T02:18:34.000Z</td>\n",
       "      <td>40095360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>1569865425084874754</td>\n",
       "      <td>\"We recommended that Congress consider compreh...</td>\n",
       "      <td>2022-09-14T01:47:38.000Z</td>\n",
       "      <td>21878488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>1569854136023404546</td>\n",
       "      <td>@philw1982 @garry_knight @PaulThomasrn @jprnyc...</td>\n",
       "      <td>2022-09-14T01:02:47.000Z</td>\n",
       "      <td>1007838595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>1569843507082428417</td>\n",
       "      <td>@OMGTheMess 20. Price point is still way too h...</td>\n",
       "      <td>2022-09-14T00:20:33.000Z</td>\n",
       "      <td>1365809989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>1569831407173906434</td>\n",
       "      <td>RT @kelrobi11: Watching @emilychangtv on Bloom...</td>\n",
       "      <td>2022-09-13T23:32:28.000Z</td>\n",
       "      <td>787053906966618112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>en</td>\n",
       "      <td>1569657195393912833</td>\n",
       "      <td>RT @Corix_JC: Consumer #data is the next virtu...</td>\n",
       "      <td>2022-09-13T12:00:12.000Z</td>\n",
       "      <td>357356770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>en</td>\n",
       "      <td>1569657066494500867</td>\n",
       "      <td>RT @Corix_JC: Consumer #data is the next virtu...</td>\n",
       "      <td>2022-09-13T11:59:42.000Z</td>\n",
       "      <td>19729690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>en</td>\n",
       "      <td>1569655901581180930</td>\n",
       "      <td>Well, here we have it, not 2 weeks after our l...</td>\n",
       "      <td>2022-09-13T11:55:04.000Z</td>\n",
       "      <td>401773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>en</td>\n",
       "      <td>1569655006642728961</td>\n",
       "      <td>RT @Vmeta3Official: VMeta3 will change how bra...</td>\n",
       "      <td>2022-09-13T11:51:31.000Z</td>\n",
       "      <td>1498038919027630081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>en</td>\n",
       "      <td>1569644488741605376</td>\n",
       "      <td>RT @SophicCapital: Read about current and prop...</td>\n",
       "      <td>2022-09-13T11:09:43.000Z</td>\n",
       "      <td>892508433315921924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang                   id  \\\n",
       "0    en  1569873208131690497   \n",
       "1    en  1569865425084874754   \n",
       "2    en  1569854136023404546   \n",
       "3    en  1569843507082428417   \n",
       "4    en  1569831407173906434   \n",
       "..  ...                  ...   \n",
       "95   en  1569657195393912833   \n",
       "96   en  1569657066494500867   \n",
       "97   en  1569655901581180930   \n",
       "98   en  1569655006642728961   \n",
       "99   en  1569644488741605376   \n",
       "\n",
       "                                                 text  \\\n",
       "0   RT @KarlBode: also for the fiftieth time the s...   \n",
       "1   \"We recommended that Congress consider compreh...   \n",
       "2   @philw1982 @garry_knight @PaulThomasrn @jprnyc...   \n",
       "3   @OMGTheMess 20. Price point is still way too h...   \n",
       "4   RT @kelrobi11: Watching @emilychangtv on Bloom...   \n",
       "..                                                ...   \n",
       "95  RT @Corix_JC: Consumer #data is the next virtu...   \n",
       "96  RT @Corix_JC: Consumer #data is the next virtu...   \n",
       "97  Well, here we have it, not 2 weeks after our l...   \n",
       "98  RT @Vmeta3Official: VMeta3 will change how bra...   \n",
       "99  RT @SophicCapital: Read about current and prop...   \n",
       "\n",
       "                  created_at            author_id  \n",
       "0   2022-09-14T02:18:34.000Z             40095360  \n",
       "1   2022-09-14T01:47:38.000Z             21878488  \n",
       "2   2022-09-14T01:02:47.000Z           1007838595  \n",
       "3   2022-09-14T00:20:33.000Z           1365809989  \n",
       "4   2022-09-13T23:32:28.000Z   787053906966618112  \n",
       "..                       ...                  ...  \n",
       "95  2022-09-13T12:00:12.000Z            357356770  \n",
       "96  2022-09-13T11:59:42.000Z             19729690  \n",
       "97  2022-09-13T11:55:04.000Z            401773333  \n",
       "98  2022-09-13T11:51:31.000Z  1498038919027630081  \n",
       "99  2022-09-13T11:09:43.000Z   892508433315921924  \n",
       "\n",
       "[600 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import json_normalize \n",
    "import pandas as pd\n",
    "twitterdf1 = json_normalize(json_response1,\"data\")\n",
    "twitterdf2 = json_normalize(json_response2,\"data\")\n",
    "twitterdf3 = json_normalize(json_response3,\"data\")\n",
    "twitterdf4 = json_normalize(json_response4,\"data\")\n",
    "twitterdf5 = json_normalize(json_response5,\"data\")\n",
    "twitterdf6 = json_normalize(json_response6,\"data\")\n",
    "twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6]\n",
    "twitterdf = pd.concat(twitterdf)\n",
    "twitterdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b5a992b9213609476f3b9dedb9c34ba69d78bca4613c4854b5108b11a72ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
